Using local[_number of cores_] for example from the documentation setMaster("local[1]") we are able to modify the number of cores available to spark 
and better runtimes as the number of cores changes. (the whole line in the code for our system is SparkSession sparkSession = SparkSession.builder().master("local[1]").appName("hpc").getOrCreate();)
 The test machine was an i7 with 4 physical cores with hyperthreding that can run simiultaions tasks on the same core so it shows up as 8 cores. 
because hyperthreading shares resources with two threads sharing the same physical processor the change between 4 cores 
and 8 was less than a second as well as expected speedup complications that come from the sequential part of the program. 

NOTE! This would be a good achedmic paper to find about hyperthreading

Spark documentation that supports above
https://spark.apache.org/docs/3.5.0/configuration.html#spark-properties
